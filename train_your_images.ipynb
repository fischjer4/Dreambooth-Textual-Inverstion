{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf user_uploaded_training_images\n",
    "!rm -rf regularization_images\n",
    "\n",
    "!mkdir user_uploaded_training_images\n",
    "!mkdir -p regularization_images/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune a stable diffusion model, we need to obtain the pre-trained stable diffusion model following their instructions. You can download the `sd-v1-4.ckpt` from google drive down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1n1vWu1EL3UveBH60bBq2HGsZR669LnZJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create a set of images for regularization, as the fine-tuning algorithm of Dreambooth requires that. Details of the algorithm can be found in the paper. Note that in the original paper, the regularization images seem to be generated on-the-fly. However, here I generated a set of regularization images before the training. The text prompt for generating regularization images can be `photo of a <class>`, where `<class>` is a word that describes the class of your object, such as `dog`. \n",
    "\n",
    "More regularization images may lead to stronger regularization and better editability. After that, save the generated images (separately, one image per .png file) at `/workspace/Dreambooth-Textual-Inverstion/regularization_images/samples`.\n",
    "\n",
    "Please try 100 or 200, to better align with the original paper.\n",
    "\n",
    "For some cases, if the generated regularization images are highly unrealistic (happens when you want to generate \"man\" or \"woman\"), you can find a diverse set of images (of man/woman) online, and use them as regularization images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload your training images to the folder named `user_uploaded_training_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_word = \"<class>\" #replace <class> with the type of subject that you are training. e.g \"person\", \"waterbottle\"\n",
    "job_name = \"<job_name>\" #replace <job_name> with the name of the job. This will be used in naming generated files for better housekeeping\n",
    "path_to_pretrained_ckpt = \"/workspace/Dreambooth-Textual-Inverstion/sd-v1-4.ckpt\" #replace this with the path to the pretrained model you downloaded from Google Drive\n",
    "unique_token_name = \"<sks>\" #replace <sks> with something that won't clash with other subjects in the model. This is the keyword you'll use in your prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're training on pictures of a man, you can use the below repository of 1500 images of a man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"man_unsplash\"\n",
    "!git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-{dataset}.git\n",
    "\n",
    "!mv -v Stable-Diffusion-Regularization-Images-{dataset}/{dataset}/*.* regularization_images/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: can't open file 'scripts/stable_txt2img.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# generate the regularization images\n",
    "!python scripts/stable_txt2img.py \\\n",
    "    --ddim_eta 0.0 \\\n",
    "    --n_samples 200 \\\n",
    "    --n_iter 1 \\\n",
    "    --scale 10.0 \\\n",
    "    --ddim_steps 50  \\\n",
    "    --ckpt {path_to_pretrained_ckpt} \\\n",
    "    --prompt f\"a photo of a {class_word}\" \\\n",
    "    --outdir \"regularization_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to `/workspace/Dreambooth-Textual-Inverstion/ldm/data/personalized.py` and replace `sks` in the line that says `photo of a sks {}` with the `unique_token_name` you set in the cell above. Something that won't clash with other subjects in the model. The final line should then say `photo of a <unique name> {}` where `<unique name>` is whatever your keyword is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf user_uploaded_training_images/.ipynb_checkpoints #clear old data if any\n",
    "\n",
    "!python main.py \\\n",
    "    --base \"configs/stable-diffusion/v1-finetune_unfrozen.yaml\" \\\n",
    "    --train \\\n",
    "    --actual_resume {path_to_pretrained_ckpt} \\\n",
    "    --name {job_name} \\\n",
    "    --data_root \"/workspace/Dreambooth-Textual-Inverstion/user_uploaded_training_images\" \\\n",
    "    --reg_data_root \"/workspace/Dreambooth-Textual-Inverstion/regularization_images/samples\" \\\n",
    "    --class_word {class_word} \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed configuration can be found in configs/stable-diffusion/v1-finetune_unfrozen.yaml. In particular, the default learning rate is 1.0e-6 as I found the 1.0e-5 in the Dreambooth paper leads to poor editability. The parameter reg_weight corresponds to the weight of regularization in the Dreambooth paper, and the default is set to 1.0.\n",
    "\n",
    "Dreambooth requires a placeholder word [V], called identifier, as in the paper. This identifier needs to be a relatively rare tokens in the vocabulary. The original paper approaches this by using a rare word in T5-XXL tokenizer. For simplicity, here I just use a random word sks and hard coded it.. If you want to change that, simply make a change in this file.\n",
    "\n",
    "Training will be run for 800 steps, and two checkpoints will be saved at `./logs/<job_name>/checkpoints`, one at 500 steps and one at final step. Typically the one at 500 steps works well enough. I train the model use two A6000 GPUs and it takes ~15 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, personalized samples can be obtained by running the command\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_newly_trained_ckpt = f\"/workspace/Dreambooth-Textual-Inverstion/logs/{job_name}/checkpoints/last.ckpt\"\n",
    "\n",
    "!python scripts/stable_txt2img.py \\\n",
    "    --ddim_eta 0.0 \\\n",
    "    --n_samples 8 \\\n",
    "    --n_iter 1 \\\n",
    "    --scale 10.0 \\\n",
    "    --ddim_steps 100 \\\n",
    "    --ckpt {path_to_newly_trained_ckpt} \\\n",
    "    --prompt f\"photo of a {unique_token_name} <class>\" \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
